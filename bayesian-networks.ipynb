{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Reasoning with Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Assignment 2 (due: Monday, 16 May, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name:\n",
    "\n",
    "Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "|             |Notes|\n",
    "|:------------|:--|\n",
    "|Maximum marks| 20|\n",
    "|Weight|20% of final grade|\n",
    "|Format| Complete this ipython notebook. Do not forget to fill in your name and student ID above|\n",
    "|Submission mode| Use [wattle](https://wattle.anu.edu.au/)|\n",
    "|Formulas| All formulas which you derive need to be explained unless you use very common mathematical facts. Picture yourself as explaining your arguments to somebody who is just learning about your assignment. With other words, do not assume that the person marking your assignment knows all the background and therefore you can just write down the formulas without any explanation. It is your task to convince the reader that you know what you are doing when you derive an argument. Typeset all formulas in $\\LaTeX$.|\n",
    "| Code quality | Python code should be well structured, use meaningful identifiers for variables and subroutines, and provide sufficient comments. Please refer to the examples given in the tutorials. |\n",
    "| Code efficiency | An efficient implementation of an algorithm uses fast subroutines provided by the language or additional libraries. For the purpose of implementing Machine Learning algorithms in this course, that means using the appropriate data structures provided by Python and in numpy/scipy (e.g. Linear Algebra and random generators). |\n",
    "| Late penalty | For every day (starts at midnight) after the deadline of an assignment, the mark will be reduced by 5%. No assignments shall be accepted if it is later than 10 days. | \n",
    "| Coorperation | All assignments must be done individually. Cheating and plagiarism will be dealt with in accordance with University procedures (please see the ANU policies on [Academic Honesty and Plagiarism](http://academichonesty.anu.edu.au)). Hence, for example, code for programming assignments must not be developed in groups, nor should code be shared. You are encouraged to broadly discuss ideas, approaches and techniques with a few other students, but not at a level of detail where specific solutions or implementation issues are described by anyone. If you choose to consult with other students, you will include the names of your discussion partners for each solution. If you have any questions on this, please ask the lecturer before you act. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has two parts. In the first part, you reason using basic rules of probability to estimate the probability and likelihood of different events (maximum 7 marks). In the second part, you implement two different approaches to perform inference on a given Bayesian Network (maximum 13 marks). All formulas and calculations which are not part of Python code should be written using $\\LaTeX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\dotprod}[2]{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\newcommand{\\onevec}{\\mathbb{1}}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed tikzmagic.py. To use it, type:\n",
      "  %load_ext tikzmagic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension as a python package.\n",
      "  \"as a python package.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "\n",
    "%install_ext https://sml.forge.nicta.com.au/isml16/tutorial/tikzmagic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext tikzmagic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reasoning and Sampling with Graphical Models\n",
    "\n",
    "### Problem setting\n",
    "\n",
    "We are interested to predict the outcome of the election in an imaginary country, called Under Some Assumptions (USA). There are four candidates for whom the citizens can **Vote** for: Bernie, Donald, Hillary, and Ted. The citizens live in four **Region**s: north, south, east and west. We have general demographic information about the people, namely: **Gender** (male, female) and **Hand**edness (right, left). Based on surveys done by an external company, we believe that the **Region** and **Gender** affects whether the people use their **Jacket**s full time, part time or never. Surprisingly, the company told us that the **Age** of their shoes (new, worn, old) depends on how often they wear their **Jacket**s. Furthermore, the **Gender** and their preferred **Hand** affects the **Colour** of their hat (white, black). Finally, surveys say that the citizens will **Vote** based on their **Region**, **Age** of their shoes and **Colour** of their hats.\n",
    "\n",
    "###  (1 mark) Draw the graphical model\n",
    "\n",
    "Based on this information, write the directed graphical model using tikz. Note that tikz code has to be in its own code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEBLAEsAAD/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB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\n5sbxjGMYxjGMYxjGM//Z\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz --scale 2 --size 700,300 -f jpg\n",
    "\\tikzstyle{vertex}=[circle, draw=black, fill=white, line width=0.5mm, minimum size=25pt, inner sep=0pt]\n",
    "\\tikzstyle{edge} = [draw, line width=1mm, ->]\n",
    "\n",
    "\\node[vertex,label=above:{Region}] (a) at (1,0) {};\n",
    "\\node[vertex,label=above:{Gender}] (b) at (-1,0) {};\n",
    "\\node[vertex,label=above:{Jacket}] (c) at (0,0) {};\n",
    "\\node[vertex,label=left:{Age}] (d) at (0,-1) {};\n",
    "\\node[vertex,label=above:{Hand}] (e) at (-2,0) {};\n",
    "\\node[vertex,label=left:{Colour}] (f) at (-1,-1) {};\n",
    "\\node[vertex,label=right:{Vote}] (g) at (0,-2) {};\n",
    "\n",
    "\n",
    "\\foreach \\source/ \\dest in {a/c, b/c, c/d, e/f, f/g, a/g, d/g, b/f}\n",
    "        \\path[edge] (\\source) -- (\\dest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probability tables\n",
    "\n",
    "After paying the survey firm some more money, they provided the following conditional probability tables.\n",
    "\n",
    "|$p(R)$ | R=n | R=s | R=e | R=w |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|marginal| 0.2 | 0.1 | 0.5 | 0.2 |\n",
    "\n",
    "|$p(G)$ | G=m | G=f |\n",
    "|:-----:|:--:|:--:|\n",
    "|marginal| 0.3 | 0.7 |\n",
    "\n",
    "|$p(H)$ | H=r | H=l |\n",
    "|:-----:|:--:|:--:|\n",
    "|marginal| 0.9 | 0.1 |\n",
    "\n",
    "| $p(J|R,G)$ | R=n,G=m | R=n,G=f | R=s,G=m | R=s,G=f | R=e,G=m | R=e,G=f | R=w,G=m | R=w,G=f |\n",
    "|:-----:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|**J**=full $\\quad$  |0.9 |0.8 |0.1 | 0.3 |0.4 |0.01| 0.02 | 0.2  |\n",
    "|**J**=part $\\quad$  |0.08|0.17|0.03| 0.35|0.05|0.01| 0.2  | 0.08 |\n",
    "|**J**=never $\\quad$ |0.02|0.03|0.87| 0.35|0.55|0.98| 0.78 | 0.72 |\n",
    "\n",
    "| $p(A|J)$ | J=full | J=part | J=never |\n",
    "|:-----:|:--:|:--:|:--:|\n",
    "|**A**=new  |0.01|0.96|0.3|\n",
    "|**A**=worn |0.98|0.03|0.5|\n",
    "|**A**=old  |0.01|0.01|0.2|\n",
    "\n",
    "| $p(C|G,H)$ | G=m,H=r | G=m,H=l | G=f,H=r | G=f,H=l |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|**C**=black $\\quad$ |0.9 |0.83 |0.17 | 0.3 |\n",
    "|**C**=white $\\quad$ |0.1 |0.17|0.83 | 0.7 |\n",
    "\n",
    "The final conditional probability table is given by the matrix below. The order of the rows are alphabetical, and the order of the columns are also given below.\n",
    "\n",
    "*Hint: The given column name format may not be the best way to code it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = ['north,new,black', 'north,new,white', 'north,worn,black', 'north,worn,white', \n",
    "                'north,old,black', 'north,old,white', 'south,new,black', 'south,new,white', \n",
    "                'south,worn,black', 'south,worn,white', 'south,old,black', 'south,old,white', \n",
    "                'east,new,black', 'east,new,white', 'east,worn,black', 'east,worn,white', \n",
    "                'east,old,black', 'east,old,white', 'west,new,black', 'west,new,white', \n",
    "                'west,worn,black', 'west,worn,white', 'west,old,black', 'west,old,white']\n",
    "VcRAC_vals = np.array([\n",
    "        [0.1,0.1,0.4,0.02,0.2,0.1,0.1,0.04,0.2,0.1,0.1 ,0.1,0.4 ,0.1 ,0.1,0.1 ,0.1,0.04,0.3,0.2,0.1,0.3,0.34,0.35],\n",
    "        [0.3,0.4,0.2,0.5 ,0.1,0.2,0.1,0.5 ,0.1,0.2,0.5 ,0.3,0.2 ,0.42,0.2,0.67,0.4,0.4 ,0.1,0.1,0.5,0.1,0.1 ,0.1],\n",
    "        [0.5,0.4,0.3,0.3 ,0.5,0.6,0.6,0.3 ,0.5,0.4,0.36,0.3,0.28,0.3 ,0.4,0.1 ,0.4,0.16,0.4,0.2,0.3,0.3,0.4 ,0.5],\n",
    "        [0.1,0.1,0.1,0.18,0.2,0.1,0.2,0.16,0.2,0.3,0.04,0.3,0.12,0.18,0.3,0.13,0.1,0.4 ,0.2,0.5,0.1,0.3,0.16,0.05]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1 mark) Encode the 7 distributions\n",
    "\n",
    "Encode the 7 conditional probability tables in python using ```pandas.Series``` and ```pandas.DataFrame```.\n",
    "\n",
    "Estimate the joint distribution of **Jacket**, **Region** and **Gender**. What is the the probability of full time **Jacket** usage among women in the west?\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encodes the marginal distributions as Series and the conditionals as DataFrames\n",
    "def encode_dists():\n",
    "    r_idx=['north', 'south','east', 'west']\n",
    "    g_idx=['male','female']\n",
    "    h_idx=['right','left']\n",
    "    j_idx=['full','part','never']\n",
    "    a_idx=['new','worn','old']\n",
    "    c_idx=['black','white']\n",
    "    v_idx=['Bernie','Donald','Hillary','Ted']\n",
    "    \n",
    "    \"Marginals\"\n",
    "    R = pd.Series([0.2, 0.1, 0.5, 0.2], index=r_idx).sort_index()\n",
    "    G = pd.Series([0.3, 0.7], index=g_idx).sort_index()\n",
    "    H = pd.Series([0.9, 0.1], index=h_idx).sort_index()\n",
    "\n",
    "    \"JcRG\"\n",
    "    rg_idx = pd.MultiIndex.from_product([r_idx, g_idx], names=['region','gender'])\n",
    "    JcRG_vals = np.array([[0.9,0.8,0.1,0.3,0.4,0.01,0.02,0.2], \n",
    "                         [0.08,0.17,0.03,0.35,0.05,0.01,0.2,0.08], \n",
    "                         [0.02,0.03,0.87,0.35,0.55,0.98,0.78,0.72]])\n",
    "    JcRG = sort_idx_cols(pd.DataFrame(JcRG_vals.T, index=rg_idx, columns=j_idx))\n",
    "\n",
    "    \"AcJ\"\n",
    "    AcJ_vals = np.array([[0.01,0.96,0.3],[0.98,0.03,0.5],[0.01,0.01,0.2]])\n",
    "    AcJ = sort_idx_cols(pd.DataFrame(AcJ_vals.T, index=j_idx, columns=a_idx))\n",
    "\n",
    "    \"CcGH\"\n",
    "    gh_idx = pd.MultiIndex.from_product([g_idx, h_idx], names=['gender','hand'])\n",
    "    CcGH_vals = np.array([[0.9, 0.83, 0.17, 0.3],[0.1, 0.17, 0.83, 0.7]])\n",
    "    CcGH = sort_idx_cols(pd.DataFrame(CcGH_vals.T, index=gh_idx, columns=c_idx))\n",
    "\n",
    "    \"VcRAC\"\n",
    "    rac_idx = pd.MultiIndex.from_product([r_idx, a_idx, c_idx], names=['region','age','colour'])\n",
    "    VcRAC = sort_idx_cols(pd.DataFrame(VcRAC_vals.T, index=rac_idx, columns=v_idx))\n",
    "    return R, G, H, JcRG, AcJ, CcGH, VcRAC\n",
    "\n",
    "# Sorts the indexes and columns for performance\n",
    "def sort_idx_cols(dist):\n",
    "        return dist.sort_index(0).sort_index(1)\n",
    "\n",
    "R, G, H, JcRG, AcJ, CcGH, VcRAC = encode_dists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a multi-index from the Cartesian product of the variables in the input list\n",
    "def multi_index(var_names_lst):\n",
    "    return pd.MultiIndex.from_product(\n",
    "        [get_idx(x) for x in var_names_lst],\n",
    "        names=var_names_lst)\n",
    "\n",
    "# Encodes a dictionary of (lexicographically) sorted variable values\n",
    "def get_idx(var_name):\n",
    "    idx_dict = { \n",
    "        'R': ['east', 'north', 'south', 'west'], \n",
    "        'G': ['female','male'],\n",
    "        'H': ['left','right'],\n",
    "        'J': ['full','never','part'],\n",
    "        'A': ['new','old','worn'],\n",
    "        'C': ['black','white'],\n",
    "        'V': ['Bernie','Donald','Hillary','Ted'],\n",
    "    }\n",
    "    return idx_dict[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>full</th>\n",
       "      <th>never</th>\n",
       "      <th>part</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <th>G</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">east</th>\n",
       "      <th>female</th>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0825</td>\n",
       "      <td>0.0075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">north</th>\n",
       "      <th>female</th>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">south</th>\n",
       "      <th>female</th>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">west</th>\n",
       "      <th>female</th>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                full   never    part\n",
       "R     G                             \n",
       "east  female  0.0035  0.3430  0.0035\n",
       "      male    0.0600  0.0825  0.0075\n",
       "north female  0.1120  0.0042  0.0238\n",
       "      male    0.0540  0.0012  0.0048\n",
       "south female  0.0210  0.0245  0.0245\n",
       "      male    0.0030  0.0261  0.0009\n",
       "west  female  0.0280  0.1008  0.0112\n",
       "      male    0.0012  0.0468  0.0120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimates the joint probability distribution over Region,Jacket and Gender\n",
    "def est_RJG():\n",
    "    rg_idx = multi_index(['R','G'])\n",
    "    RJG = pd.DataFrame(np.ones((8,3)),rg_idx, columns=get_idx('J'))\n",
    "    RJG.sort_index(inplace=True)\n",
    "    for i in rg_idx:\n",
    "            for j in get_idx('J'):\n",
    "                RJG.loc[i][j] = float(JcRG.loc[i][j]*R.loc[i[0]]*G.loc[i[1]])\n",
    "    return RJG\n",
    "display(est_RJG())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Probability of full time Jacket usage among Women in the West **\n",
    "* The syntax of the question suggests that we desire the conditional P(J = full | G = female, R = west). 'Among' suggests that the sample space should be limited to women in the west.\n",
    "* However, the context of the question suggests the joint P(J = full, G = female, R = west) since we have just calculated this distribution.\n",
    "\n",
    "I provide both below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional P(J = full | G = female, R = west) =  0.2\n",
      "Joint P(J = full, G = female, R = west) =  0.028\n"
     ]
    }
   ],
   "source": [
    "print('Conditional P(J = full | G = female, R = west) = ', JcRG.loc['west','female']['full'])\n",
    "print('Joint P(J = full, G = female, R = west) = ', est_RJG().loc['west','female']['full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 marks) Naive sampling approach\n",
    "\n",
    "Implement the following sampling scheme:\n",
    "1. Order the nodes in the following fashion, R, G, H, J, A, C, V. Work through each node in order.\n",
    "2. For the first node R, draw a sample from p(R).\n",
    "3. For each subsequent node, draw a sample from the conditional distribution $p(X \\,|\\, parents(X))$ where $parents(X)$ are the parents of the variable $X$ in the graphical model.\n",
    "\n",
    "Your code should be modular, with appropriate use of (well documented) functions.\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encodes the Graph as a dict and calls the recursive sample function\n",
    "# Returns the sampled values for each variable as a dict\n",
    "# *** Note, the values are the PARENTS of the key node, not the children ***\n",
    "def sampler_driver():\n",
    "    bayes_net = {\n",
    "        'R': (R, None), \n",
    "        'G': (G, None), \n",
    "        'H': (H, None), \n",
    "        'J': (JcRG, ['R','G']), \n",
    "        'A': (AcJ, ['J']),\n",
    "        'C': (CcGH, ['G','H']),\n",
    "        'V': (VcRAC, ['R','A','C'])\n",
    "    }\n",
    "    samples = {\n",
    "        'R': None, \n",
    "        'G': None, \n",
    "        'H': None, \n",
    "        'J': None, \n",
    "        'A': None,\n",
    "        'C': None,\n",
    "        'V': None,\n",
    "    }\n",
    "    return sample('V', samples, bayes_net)\n",
    "\n",
    "# Recursive sampler function, implements ancestral sampling\n",
    "# Recursively calls itself on each of the parents of the input node\n",
    "# until the marginal node R is reached. Sampled values of parent nodes propagate\n",
    "# back through the recursion until all nodes have been sampled from.\n",
    "def sample(node, samples, graph):\n",
    "    pars = graph[node][1]\n",
    "    if pars == None:\n",
    "        samples[node] = np.random.choice(graph[node][0].index, \n",
    "                                   p=graph[node][0].values)\n",
    "        return samples\n",
    "    else:\n",
    "        for parent in pars:\n",
    "            if (samples[parent] == None):\n",
    "                samples[parent] = sample(parent, samples, graph)[parent]\n",
    "        par_samps = tuple([samples[x] for x in pars])\n",
    "        samples[node] = np.random.choice(graph[node][0].columns,\n",
    "                                         p=graph[node][0].loc[par_samps])\n",
    "        return samples\n",
    "\n",
    "# Calculates n samples from the graph\n",
    "def get_n_samples(n):\n",
    "    sample_list = []\n",
    "    for i in range(n):\n",
    "        sample_list.append(sampler_driver())\n",
    "    return pd.DataFrame(sample_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler provides a sample from the joint distribution of all random variables. Why was the order in point 1. chosen? Are there other possible orders which achieve the same sampler? If yes, specify another one.\n",
    "### Solution\n",
    "* The order is a topological ordering of the graph, so that no node has an incoming edge from a node that appears later in the ordering. This is necessary in ancestral (naive) sampling because incoming edges represent conditions, and the conditional probabilities of later nodes can't be calculated if the nodes (variables) they are conditioned on have not been sampled from.\n",
    "* Yes, a topological ordering is not unique. Alternative: G R H J A C V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a small test case to check that your sampler is doing the right thing by using it to verify the sum rule and product rule of probability. You can do so by drawing many samples from both sides of each rule, and show that the proportion of each state is as expected.\n",
    "### Solution\n",
    "We will test using variables G and H, using two sets of samples, sample_A for LHS of rules and sample_B for RHS. We first define several functions that will be used in ensuing questions to calculate distributions from data/samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generates a mask for the DataFrame where the vars in lst_tuples\n",
    "# equal the corresponding value in lst_tuples\n",
    "def gen_mask(data, lst_tuples):\n",
    "    bool_list = []\n",
    "    for item in lst_tuples:\n",
    "        bool_list.append(data[item[0]] == item[1])\n",
    "    mask = [all(tup) for tup in zip(*bool_list)]\n",
    "    return mask\n",
    "\n",
    "# Helper function to deal with single/multiple variables\n",
    "def get_values(i,j, lst_cond_names):\n",
    "    if type(i) != tuple:\n",
    "        cond_values = [i]\n",
    "    else: \n",
    "        cond_values = list(i)\n",
    "    if type(j) != tuple:\n",
    "        jnt_values = cond_values + [j]\n",
    "    else:\n",
    "        jnt_values = cond_values + list(j)\n",
    "    return jnt_values, cond_values\n",
    "\n",
    "# Empirically estimates the distribution over jnt_vars given cond_vars\n",
    "def empir_cond(data, jnt_vars, cond_vars):\n",
    "    jnt_idx = multi_index(jnt_vars)\n",
    "    cond_idx = multi_index(cond_vars)\n",
    "    empir_cond = pd.DataFrame(np.zeros((len(cond_idx),len(jnt_idx))), index=cond_idx, columns=jnt_idx)\n",
    "    all_vars = cond_vars + jnt_vars\n",
    "    for i in cond_idx:\n",
    "        for j in jnt_idx:\n",
    "            jnt_values, cond_values = get_values(i, j, cond_vars)\n",
    "            mask_1 = gen_mask(data, [tup for tup in zip(cond_vars, cond_values)])\n",
    "            mask_2 = gen_mask(data, [tup for tup in zip(all_vars, jnt_values)])       \n",
    "            empir_cond.loc[i][j] = len(data[mask_2])/len(data[mask_1])\n",
    "    return empir_cond\n",
    "\n",
    "# Empirically estimates the marginal probability of var_name\n",
    "def empir_marg(data, var_name):\n",
    "    var_idx = get_idx(var_name)\n",
    "    marg = pd.Series(np.zeros(len(var_idx)), var_idx)\n",
    "    len_all = len(data[var_name])\n",
    "    for x in var_idx:\n",
    "        marg.loc[x] = len(data[data[var_name]== x])/len_all\n",
    "    return marg\n",
    "\n",
    "# Empirically estimates the joint probability of the vars in joint_vars\n",
    "def empir_joint(data, joint_vars):\n",
    "    joint_idx = multi_index(joint_vars)\n",
    "    joint_df = pd.DataFrame(np.zeros(len(joint_idx)), joint_idx)\n",
    "    len_all = len(data)\n",
    "    for i in joint_idx:\n",
    "        mask = gen_mask(data, [tup for tup in zip(joint_vars, list(i))])\n",
    "        joint_df.loc[i] = len(data[mask])/len_all\n",
    "    return joint_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_A = get_n_samples(4000)\n",
    "sample_B = get_n_samples(4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Rule: $P(G,H) = P(G | H)P(H)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prod Rule LHS: P(G,H)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "G       H    \n",
       "female  left     0.07075\n",
       "        right    0.62350\n",
       "male    left     0.02675\n",
       "        right    0.27900\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prod Rule RHS: P(G|H)P(H)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "G       H    \n",
       "female  left     0.07675\n",
       "        right    0.62350\n",
       "male    left     0.03075\n",
       "        right    0.26900\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Est. of joint P(G,H) from sample_A. (LHS of Product Rule)\n",
    "def est_joint(sample):\n",
    "    gh_idx = multi_index(['G','H'])\n",
    "    GH_joint=pd.Series(np.zeros(4), gh_idx)\n",
    "    for i in gh_idx:\n",
    "            df = sample[['G','H']][(sample.H == i[1])]\n",
    "            GH_joint[i] = len(df.G[df.G == i[0]])/len(sample)\n",
    "    return GH_joint\n",
    "\n",
    "# Est. of P(G|H)P(H) from sample_B (RHS of Product Rule)\n",
    "def est_prod_RHS(sample):\n",
    "    GcH = empir_cond(sample_B, ['G'],['H'])\n",
    "    H = empir_marg(sample_B, 'H')\n",
    "    GcH_H = GcH.T.mul(H).as_matrix().flatten()\n",
    "    return pd.Series(GcH_H, multi_index(['G','H']))\n",
    "\n",
    "print(\"Prod Rule LHS: P(G,H)\")\n",
    "display(est_joint(sample_A))\n",
    "print(\"Prod Rule RHS: P(G|H)P(H)\")\n",
    "display(est_prod_RHS(sample_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum Rule: $P(G) = \\sum_{H} P(G,H)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Rule LHS: P(G)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "female    0.69425\n",
       "male      0.30575\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Rule RHS: Sum_{H} P(G,H)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "female    0.70025\n",
       "male      0.29975\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Est. of Sum_{H}P(G,H) from sample_A. (RHS of Sum Rule)\n",
    "print(\"Sum Rule LHS: P(G)\")\n",
    "display(empir_marg(sample_A,'G'))\n",
    "\n",
    "# Est. of marginal P(G) from sample_B. (LHS of Sum Rule)\n",
    "def est_RHS_sum_rule(sample_B):\n",
    "    G = pd.Series(np.zeros(2), get_idx('G'))\n",
    "    for g in get_idx('G'):\n",
    "        for h in get_idx('H'):\n",
    "            G[g] += est_joint(sample_B)[g,h]\n",
    "    return G\n",
    "print(\"Sum Rule RHS: Sum_{H} P(G,H)\")\n",
    "display(est_RHS_sum_rule(sample_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2 marks) Estimating conditional probabilities\n",
    "\n",
    "Use your sampler above to estimate:\n",
    "1. the marginal probability of $p(V)$,\n",
    "2. the probability of women that vote for Hillary, $p(V=Hillary \\,|\\, G=female)$.\n",
    "\n",
    "Show a plot that describes the number of samples that are needed to estimate $p(V)$ accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Estimate of Marginal P(V):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bernie     0.15675\n",
       "Donald     0.35375\n",
       "Hillary    0.28700\n",
       "Ted        0.20250\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Est. of P(V = Hillary | G = female): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25063017644940583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"1. Estimate of Marginal P(V):\")\n",
    "display(empir_marg(sample_A, 'V'))\n",
    "print(\"2. Est. of P(V = Hillary | G = female): \")\n",
    "display(empir_cond(sample_A, ['V'], ['G']).loc['female']['Hillary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draws n samples to estimate the marginal probability of V\n",
    "def est_V_n_samples(n):\n",
    "    samples = get_n_samples(n)\n",
    "    V = empir_marg(samples, 'V')\n",
    "    return V\n",
    "\n",
    "# Generates a series of estimates and variance values for P(V)\n",
    "# for plotting. Each set of n samples is drawn independently\n",
    "def gen_plot_data():\n",
    "    V_df = []\n",
    "    var_df = []\n",
    "    n_s = np.arange(1,300)\n",
    "    for n in n_s:\n",
    "        V_est = est_V_n_samples(n)\n",
    "        V_df.append(V_est)\n",
    "        if n <= 10:\n",
    "            var = np.var(V_df[0:n],axis=0)\n",
    "            var_df.append(var)\n",
    "        else:\n",
    "            var_last_10 = np.var(V_df[n-10:n],axis=0)\n",
    "            var_df.append(var_last_10)\n",
    "    V_df = pd.DataFrame(V_df, columns=get_idx('V'))\n",
    "    var_df = pd.DataFrame(var_df, columns=get_idx('V'))\n",
    "    return V_df, var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V_df, var_df = gen_plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plots the data generated\n",
    "var_plot_df = var_df.loc[0:50]\n",
    "var_plot = var_plot_df.plot(kind='line')\n",
    "var_plot.set_xlabel('no. of samples')\n",
    "var_plot.set_ylabel('Variance of estimate of P(V)')\n",
    "V_plot = V_df.plot(kind='line')\n",
    "V_plot.set_xlabel('no. of samples')\n",
    "V_plot.set_ylabel('P(V)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with 1 sample and then increase 1 sample at a time to a maximum of 300 samples. The first plot shows the variance among the last 10 estimates of P(V), which is high early, and is < 0.01 after only 50 independent samples, though this is not an unusual result (cf. Bishop p524). The second plot shows the estimate of P(V) each time n samples are taken, which progressively stabilises (and in the limit should converge to the real P(V) if our samples truly are independent, cf. Bishop p524)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 marks) Direct estimation of conditional probabilities\n",
    "\n",
    "Write down the expression of the joint probability $p(R,G,H,J,A,C,V)$ in terms of the conditional probabilities in the graphical model.\n",
    "\n",
    "Use the sum rule and product rule of probability to derive:\n",
    "* $p(V = Donald \\;\\,|\\, G = male)$\n",
    "* $p(G = male \\,|\\, V = Donald)$\n",
    "* the marginal probability of **Vote**s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Joint Probability:\n",
    "$$p(R,G,H,J,A,C,V) = p(R)p(G)p(H)p(J|R,G)p(A|J)p(C|GH)p(V|R,A,C)$$\n",
    "(a)\n",
    "\\begin{align}\n",
    "p(V|G) = \\frac{p(V,G)}{P(G)} &= \\frac{\\sum_{R,H,J,A,C}p(R,G,H,J,A,C,V)}{p(G)}\\\\\n",
    "&= \\frac{\\sum_{R,H,J,A,C}p(R)p(G)p(H)p(J|R,G)p(A|J)p(C|GH)p(V|R,A,C)}{p(G)}\\\\\n",
    "&= \\sum_{R,H,J,A,C}p(R)p(H)p(J|R,G)p(A|J)p(C|GH)p(V|R,A,C)\\\\\n",
    "&= \\sum_{R}p(R)\\sum_{J}p(J|R,G)\\sum_{A}p(A|J)\\sum_{C}p(V|R,A,C)\\sum_{H}p(C|G,H)p(H)\n",
    "\\end{align}\n",
    "Hence\n",
    "\\begin{align}\n",
    "p(V=D|G=M)\n",
    "&= \\sum_{R}p(R)\\sum_{J}p(J|R,G=M)\\sum_{A}p(A|J)\\sum_{C}p(V=D|R,A,C)\\sum_{H}p(C|G=M,H)p(H)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) \n",
    "\\begin{align}\n",
    "p(G=M|V=D)\n",
    "&= \\frac{p(V=D|G=M)p(G=M)}{p(V=D)}\\\\\n",
    "&=\\frac{p(G=M)\\sum_{R}p(R)\\sum_{J}p(J|R,G=M)\\sum_{A}p(A|J)\\sum_{C}p(V=D|R,A,C)\\sum_{H}p(C|G=M,H)p(H)}{p(V=D)}\\\\\n",
    "&=\\frac{p(G=M)\\sum_{R}p(R)\\sum_{J}p(J|R,G=M)\\sum_{A}p(A|J)\\sum_{C}p(V=D|R,A,C)\\sum_{H}p(C|G=M,H)p(H)}{\\sum_{R}p(R)\\sum_{G}p(G)\\sum_{J}p(J|R,G)\\sum_{A}p(A|J)\\sum_{C}p(V=D|R,A,C)\\sum_{H}p(C|G=M,H)p(H)}\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) \n",
    "\\begin{align}\n",
    "p(V) = \\sum_{R,G,H,J,A,C}p(R,G,H,J,A,C,V) &= \\sum_{R,G,H,J,A,C}p(R)p(G)p(H)p(J|R,G)p(A|J)p(C|GH)p(V|R,A,C) \\\\\n",
    "&= \\sum_{R}p(R)\\sum_{G}p(G)\\sum_{J}p(J|R,G)\\sum_{A}p(A|J)\\sum_{C}p(V|R,A,C)\\sum_{H}p(H)p(C|G,H)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the encoding of the conditional probability tables in ```pandas.Series``` and ```pandas.DataFrame```, and your expressions above, calculate the desired conditional probability tables. Your code should be modular, with appropriate use of (well documented) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix multiplies the factors (conditional distributions)\n",
    "# to retrieve P(V|G). Uses the Einstein summation notation to simplify\n",
    "# the representation of the operations. \n",
    "# *** Not all of the intermediate results calculated are proper distributions ***\n",
    "# *** They should be thought of as functions of the variables in their names ***\n",
    "def VcG():\n",
    "    CcG = np.einsum('cgh,h->gc', CcGH.to_panel(), H)\n",
    "    CVRA = np.array([VcRAC.xs('black', level='colour').to_panel().as_matrix(),\n",
    "                     VcRAC.xs('white', level='colour').to_panel().as_matrix()])\n",
    "    VRAG = np.einsum('cvra,gc->vrag', CVRA, CcG)\n",
    "    VRGJ = np.einsum('vrag,ja->vrgj', VRAG, AcJ)\n",
    "    VcRG = np.einsum('vrgj,jrg->vrg',VRGJ,JcRG.to_panel())\n",
    "    VcG = np.einsum('vrg,r->gv',VcRG,R)\n",
    "    return pd.DataFrame(VcG, get_idx('G'), columns=get_idx('V'))\n",
    "\n",
    "# Calculates P(G|V)\n",
    "def GcV():\n",
    "    GcV = np.einsum('gv,g,v->vg', VcG(),G,1/V_marg())\n",
    "    return pd.DataFrame(GcV, get_idx('V'), columns=get_idx('G'))\n",
    "\n",
    "# Calculates P(V)\n",
    "def V_marg():\n",
    "    return VcG().T.dot(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('p(V=D|G=M) = ')\n",
    "display(VcG().loc['male']['Donald'])\n",
    "print('\\np(G=M|V=D) = ')\n",
    "display(GcV().loc['Donald']['male'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|$p(V | G)$ | V = Bernie | V = Donald | V = Hillary | V = Ted |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|G = female | 0.133114 | 0.387251 | 0.268675 | 0.210960 |\n",
    "|G = male| 0.205460 | 0.246485 | 0.358109 | 0.189947 |\n",
    "\n",
    "\n",
    "|$p(V)$ | V = Bernie | V = Donald | V = Hillary | V = Ted |\n",
    "|:-----:|:--:|:--:|:--:|:--:|\n",
    "|marginal| 0.154818 | 0.345021 | 0.295505 | 0.204656 |\n",
    "\n",
    "\n",
    "|$p(G|V)$ | G = female| G = male |\n",
    "|:-----:|:--:|:--:|\n",
    "| V = Bernie | 0.601867 | 0.398133 | \n",
    "| V = Donald | 0.785678 | 0.214322 | \n",
    "| V = Hillary | 0.636444 | 0.363556  |\n",
    "| V = Ted | 0.721562 | 0.278438 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 marks) Estimating structure\n",
    "\n",
    "It turns out that the external company that we used to estimate the probabilities above did not do such a good job. There is reason to believe that the given graphical model above is wrong. However, it is believed that the true graphical model has more or less the same number of edges, and is sharing some conditional probability tables. In particular, assume that the conditional probability table for **Voting** conditioned on **Region**, **Age** and **Colour** remains the same. \n",
    "\n",
    "An enterprising student from the Amazing Neighbourhood University (ANU) obtained some data by polling. This data is available from [this website](https://sml.forge.nicta.com.au/isml16/assignment/poll-data.csv.gz). Use this data to identify a good graphical model.\n",
    "\n",
    "Show:\n",
    "1. The tikz image of the graphical model (this has to be in its own cell)\n",
    "2. The conditional probability tables as markdown\n",
    "\n",
    "Argue why the new graphical model that you estimated from the data is a good one.\n",
    "\n",
    "*A majority of marks are allocated for the method of finding the graphical model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Graph Structure Algorithm *** \n",
    "* The algorithm first initialises a dict (adjacency list) representation of the graph, with keys being nodes and values being the list of (edges to) PARENTS of the key node. The edges between V and R, A, and C are added, since we are told this conditional probability table is correct. All other adjacency lists are blank.\n",
    "* For every remaining pair of variables, the mean of the pairwise KL divergences between $P(A|B = b)$, and $P(A|B = b')$, for all $b, b'$. This is an efficient way of measuring the divergence (or 'distance') between P(A|B) for different values of the condition B. If this mean value is close to 0, conditioning on B has little effect. The variables are therefore concluded to be independent, and no edge is added between them.\n",
    "* For each of the dependent pairs, an undirected edge is added to the graph.\n",
    "* To make a principled choice regarding what 'close to 0' means, we compute the KL divergence between the empirical estimate of $P(V|RAC)$, and the given true conditional probability table (since we are told this table is correct). This gives us an estimate of the divergence we can expect to result simply from noise in the data and sampling error. We multiply this divergence by a tolerance factor to get the tolerance that will be used when KL divergences are calculated by all remaining methods. A higher tolerance factor allows for more conditional independences to be found (since two almost equal distributions will be treated equal), and thus encourages a sparser graph. A very low tolerance factor will cause the algorithm to simply return the joint simply factorised according to the product rule. This tolerance factor therefore fucntions as a regularisation parameter over the number of edges in the graph.\n",
    "\n",
    "*** Reasoning Using Blocked Paths ***\n",
    "* Next, the algorithm looks at junctions in the graph (triplets of nodes with connecting edges). If the nodes adjacent to the junction are not connected, they are conditionally independent given nothing, so the junction must be a Head-Head node, which allows the determination of the direction of the relevant edges.\n",
    "* If the nodes A and C adjacent to the junction node B have a connecting edge, they are dependent given nothing (as per the earlier part of the algorithm). We therefore conditional independence of A and C given B. If A is conditionally independent of C given B, the connecting edge between A and B is removed. \n",
    "\n",
    "*** Assigning Direction to Remaining Undirected Edges, Testing for Cycles ***\n",
    "* At this stage, the graph properties already tested entail that most edges are already directed. In fact, since no higher order conditional independences can remain for this particular graph (no remaining variables are conditioned on more than two variables), we can conclude that the direction of all remaining edges is arbitrary (because the factorisations corresponding to either choice of direction will be equivalent. That is, unless, there are cycles remaining in the graph.\n",
    "* We therefore randomly assign directions to the remaining undirected edges, and test if the graph is a directed acyclic graph DAG. The algorithm terminates when the first DAG is found.\n",
    "\n",
    "*** Evaluating the Graph ***\n",
    "* We then compute the factored joint distribution by multiplying the factors corresponding to the derived graph.\n",
    "* Then we take the KL divergence between this graph and the empirically estimated joint distribution. If it is sufficiently close to the tolerance calculated earlier, we conclude that this factorization accurately represents the distribution of the data.\n",
    "* After running the algorithm for various tolerances near our calculated tolerance, only two graphs emerged as candidates: a sparser graph and a denser one. We computed the KL divergence between each corresponding factorisation and the empirical joint distribution, and between the two factorised joints, and found the KLs to be basically equal. This means that they both represent the underlying distribution equally well, and thus the sparser graph must capture the true conditional independences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generates the powerset of parents for a node, \n",
    "# size of subsets can also be specified\n",
    "def powerlist(iterable, subset_size = None):\n",
    "    s = list(iterable)\n",
    "    plist =  list(itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "    if subset_size != None:\n",
    "        return [tup for tup in plist if len(tup) == subset_size]\n",
    "    else: return [tup for tup in plist if len(tup) > 0 and len(tup) < len(s)]\n",
    "\n",
    "# Calculates the KL divergence tolerance by comparing the empirically estimate\n",
    "# of P(V|RAC) with the given true probability table \n",
    "def KL_tol(data, VcRAC):\n",
    "    empir_VcRAC = empir_cond(data, ['V'], ['R','A','C'])\n",
    "    KL = 0\n",
    "    rac_idx = multi_index(['R','A','C'])\n",
    "    KL = entropy(VcRAC, empir_VcRAC).mean()\n",
    "    return KL\n",
    "\n",
    "def prune_list(iterable, ignore_list):\n",
    "    return [item for item in iterable if item not in ignore_list]\n",
    "\n",
    "# Calculates the pairwise KL divergences between P(A|B = b) and P(A|B=b')\n",
    "# for different values of the condition, to determine independence K\n",
    "def pairw_KL(data, var_tup):\n",
    "    cond_vals = get_idx(var_tup[1])\n",
    "    cond = empir_cond(data, [var_tup[0]],[var_tup[1]])\n",
    "    marg = empir_marg(data, var_tup[0])\n",
    "    KL = 0\n",
    "    for val in cond_vals:\n",
    "        KL += entropy(marg, cond.loc[val])\n",
    "    return KL/len(cond_vals)\n",
    "\n",
    "# Calculates the pairwise KLs for all variables in the graph\n",
    "def all_pairw_KL(data, var_names):\n",
    "    pairs = powerlist(var_names,2)\n",
    "    pairs = prune_list(pairs, list(itertools.product(['R','A','C'],['V'])))\n",
    "    return dict(zip(pairs, [pairw_KL(data, pair) for pair in pairs]))\n",
    "\n",
    "# Removes the independent pairs of variables from the list of pairs\n",
    "def rm_indep_pairs(KL_d, KL_tol):\n",
    "    hi_KL_d = dict(KL_d)\n",
    "    for key in KL_d.keys():\n",
    "        if KL_d[key] < KL_tol:\n",
    "            del_d_item(hi_KL_d, key)\n",
    "    return hi_KL_d\n",
    "\n",
    "# Adds edges between dependent variables to the graph\n",
    "def add_dep_edges(graph, low_KL_dict):\n",
    "    for key in low_KL_dict.keys():\n",
    "        graph[key[0]] = graph[key[0]] + [key[1]]\n",
    "        graph[key[1]] = graph[key[1]] + [key[0]]\n",
    "    return graph\n",
    "\n",
    "def del_d_item(d, key):\n",
    "    del d[key]\n",
    "    return d\n",
    "\n",
    "# Uses the blocked path properties of graphical models to\n",
    "# remove edges and determine the direction of edges\n",
    "def use_blocked_paths(data, undir_graph, KL_tol):\n",
    "    for key in undir_graph.keys():\n",
    "        junction_list = powerlist(undir_graph[key],2)\n",
    "        for tup in junction_list:\n",
    "            if tup[0] in undir_graph[tup[1]]:\n",
    "                undir_graph = check_c_indep(data, undir_graph, key, tup, KL_tol)\n",
    "            else: \n",
    "                undir_graph = make_HH(undir_graph, key, tup)\n",
    "    return undir_graph\n",
    "\n",
    "# Fixes the direction of certain edges if the junction is Head-Head\n",
    "def make_HH(graph, node, parents):\n",
    "    for i in range(2):\n",
    "        if(node in graph[parents[i]]):\n",
    "            graph[parents[i]].remove(node)\n",
    "    return graph\n",
    "\n",
    "# Checks for conditional independence between the vars in parents given node\n",
    "def check_c_indep(data, graph, node, parents, KL_tol):\n",
    "    if three_w_KL(data, graph, node, parents) < KL_tol:\n",
    "        if parents[0] in graph[parents[1]]:\n",
    "            graph[parents[1]].remove(parents[0])\n",
    "        if parents[1] in graph[parents[0]]:\n",
    "            graph[parents[0]].remove(parents[1])\n",
    "    return graph    \n",
    "\n",
    "# Computes the KL divergence between P(A|BC) and P(A|C) to check\n",
    "# conditional independence\n",
    "def three_w_KL(data, graph, node, parents):\n",
    "    AcB = empir_cond(data, [parents[0]], [node])\n",
    "    AcBC = empir_cond(data, [parents[0]], [node,parents[1]])\n",
    "    B_vals = get_idx(node)\n",
    "    C_vals = get_idx(parents[1])\n",
    "    KL = 0\n",
    "    BC_joint = empir_joint(data, [node, parents[1]])\n",
    "    for b in B_vals:\n",
    "        for c in C_vals:\n",
    "            KL += entropy(AcB.loc[b], AcBC.loc[b,c]) * BC_joint.loc[b,c]\n",
    "    return KL.mean()\n",
    "\n",
    "# Removes undirected edges and checks for cycles until a DAG is found\n",
    "def rm_ud_edges(ud_graph):\n",
    "    ud_edges = []\n",
    "    for key in ud_graph:\n",
    "        for parent in ud_graph[key]:\n",
    "            if key in ud_graph[parent]:\n",
    "                if tuple([parent, key]) not in ud_edges:\n",
    "                    ud_edges.append(tuple([key,parent]))\n",
    "    graph_A = copy.deepcopy(ud_graph)\n",
    "    for edge in ud_edges:\n",
    "        if edge[0] in ud_graph[edge[1]]:\n",
    "            graph_A[edge[1]].remove(edge[0])\n",
    "    if is_DAG(graph_A):\n",
    "        return graph_A\n",
    "    graph_B = copy.deepcopy(ud_graph)\n",
    "    for edge in ud_edges:\n",
    "        if edge[1] in ud_graph[edge[0]]:\n",
    "            graph_B[edge[0]].remove(edge[1])\n",
    "    if is_DAG(graph_B):\n",
    "        return graph_B\n",
    "\n",
    "def is_DAG(g):\n",
    "    \"\"\"\n",
    "    Returns True if graph is a directed acyclic graph.\n",
    "    This function was taken from \n",
    "    http://codereview.stackexchange.com/questions/86021/check-if-a-directed-graph-contains-a-cycle\n",
    "    at the suggestion of another student (Suraj Sasikumar) who needed a similar function\n",
    "    \"\"\"\n",
    "    path = set()\n",
    "    def visit(node):\n",
    "        path.add(node)\n",
    "        for parent in g[node]:\n",
    "            visit(parent)\n",
    "            if parent in path:\n",
    "                return False\n",
    "            if not visit(parent):\n",
    "                return False\n",
    "        path.remove(node)\n",
    "        return True\n",
    "    return all(visit(node) for node in g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes the joint distribution from the factorisation of the denser graph found\n",
    "def factor_jnt_dense(data, name_list):\n",
    "    idx = multi_index(name_list)\n",
    "    factor_jnt = pd.DataFrame(np.zeros(len(idx)), idx, columns=['P'])\n",
    "    C = empir_marg(data, 'C')\n",
    "    H = empir_marg(data, 'H')\n",
    "    R = empir_marg(data, 'R')\n",
    "    GcCJ = empir_cond(data, ['G'],['C','J'])\n",
    "    AcR = empir_cond(data, ['A'], ['R'])\n",
    "    JcC = empir_cond(data, ['J'],['C'])\n",
    "    for i in idx:\n",
    "        factor_jnt.loc[i]['P'] = R.loc[i[0]]*H.loc[i[2]]*C.loc[i[5]]*GcCJ.loc[i[5],i[3]][i[1]]*JcC.loc[i[5]][i[3]]*AcR.loc[i[0]][i[4]]*VcRAC.loc[i[0],i[4],i[5]][i[6]]\n",
    "    return factor_jnt\n",
    "\n",
    "# Computes the joint distribution from the factorisation of the sparser graph found\n",
    "def factors_jnt_sparse(data, name_list):\n",
    "    idx = multi_index(name_list)\n",
    "    factor_jnt = pd.DataFrame(np.zeros(len(idx)), idx, columns=['P'])\n",
    "    C = empir_marg(data, 'C')\n",
    "    H = empir_marg(data, 'H')\n",
    "    R = empir_marg(data, 'R')\n",
    "    G = empir_marg(data, 'G')\n",
    "    JcCG = empir_cond(data, ['J'],['C','G'])\n",
    "    AcR = empir_cond(data, ['A'], ['R'])\n",
    "    for i in idx:\n",
    "        factor_jnt.loc[i]['P'] = R.loc[i[0]]*H.loc[i[2]]*G.loc[i[1]]*C.loc[i[5]]*JcCG.loc[i[5],i[1]][i[3]]*AcR.loc[i[0]][i[4]]*VcRAC.loc[i[0],i[4],i[5]][i[6]]\n",
    "    return factor_jnt\n",
    "\n",
    "# Computes the KL divergences between the factorised joints and the empirically estimated joint\n",
    "def KLs_factor_jnts(data, name_list):\n",
    "    jnt_dense_g = factor_jnt_dense(data, name_list)\n",
    "    jnt_sparse_g = factors_jnt_sparse(data, name_list)\n",
    "    empr_joint = empir_joint(data, name_list)\n",
    "    KL_dense = entropy(empr_joint,jnt_dense_g)\n",
    "    print(\"KL divergence between empirical joint and factored joint for dense graph: \", KL_dense)\n",
    "    KL_sparse = entropy(empr_joint,jnt_sparse_g)\n",
    "    print(\"KL divergence between empirical joint and factored joint for sparse graph: \", KL_sparse)\n",
    "    KL_rel = entropy(jnt_dense_g,jnt_sparse_g)\n",
    "    print(\"KL divergence between factored joints for sparse and dense graph: \", KL_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    name_list = ['R','G','H','J','A','C','V'] \n",
    "    data = pd.read_csv('poll-data.csv', names = name_list).loc[1:]\n",
    "    return data, name_list\n",
    "\n",
    "# MAIN FUNCTION FOR COMPUTING OPTIMAL GRAPH\n",
    "# Optimal graph found depends on the tolerance KL_tol\n",
    "def optimal_graph(data, KL_tol, name_list):\n",
    "    base_graph = dict(zip(name_list, list(itertools.repeat([], len(name_list)))))\n",
    "    base_graph['V'] = ['R','A','C']\n",
    "    pairw_KLs = all_pairw_KL(data, name_list)\n",
    "    dep_pairs = rm_indep_pairs(pairw_KLs, KL_tol)\n",
    "    ud_graph = add_dep_edges(base_graph, dep_pairs)\n",
    "    ud_graph = use_blocked_paths(data, ud_graph, KL_tol)\n",
    "    opt_graph = rm_ud_edges(ud_graph)\n",
    "    return opt_graph\n",
    "\n",
    "# DRIVER FUNCTION: \n",
    "# Runs the optimal graph finder, and calls methods for displaying graphs and tables\n",
    "# and computing the KL divergences to evaluate the graphs\n",
    "def opt_graph_driver(VcRAC):\n",
    "    data, name_list = read_data()\n",
    "    tol = KL_tol(data,VcRAC)\n",
    "    opt_g_low_edges = optimal_graph(data, tol*4, name_list)\n",
    "    opt_g_hi_edges = optimal_graph(data, tol*2, name_list)\n",
    "    print(\"Graph for low KL divergence tolerance: \")\n",
    "    display(opt_g_hi_edges)\n",
    "    print(\"Graph for high KL divergence tolerance: \")\n",
    "    display(opt_g_low_edges)\n",
    "    KLs_factor_jnts(data, name_list)\n",
    "    print(\"\\nWe therefore choose the sparser graph and present it and its conditional probability tables below: \")\n",
    "    display(empir_cond(data, ['J'],['G','C']))\n",
    "    display(empir_cond(data, ['R'],['A']))\n",
    "    display(empir_marg(data, 'A'))\n",
    "    display(empir_marg(data, 'G'))\n",
    "    display(empir_marg(data, 'H'))\n",
    "    display(empir_marg(data, 'C'))\n",
    "    print('Note: the conditional table for P(V|RAC) is omitted as it remains the same.')\n",
    "    return opt_g_hi_edges, opt_g_low_edges\n",
    "    \n",
    "opt_g_low_edges, opt_g_hi_edges = opt_graph_driver(VcRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%tikz --scale 2 --size 700,300 -f jpg\n",
    "\\tikzstyle{vertex}=[circle, draw=black, fill=white, line width=0.5mm, minimum size=25pt, inner sep=0pt]\n",
    "\\tikzstyle{edge} = [draw, line width=1mm, ->]\n",
    "\\node[vertex,label=above:{Region}] (r) at (1,-1) {};\n",
    "\\node[vertex,label=above:{Gender}] (g) at (-3,-1) {};\n",
    "\\node[vertex,label=right:{Jacket}] (j) at (-2,-2) {};\n",
    "\\node[vertex,label=above:{Age}] (a) at (0,-1) {};\n",
    "\\node[vertex,label=above:{Hand}] (h) at (-2,-1) {};\n",
    "\\node[vertex,label=above:{Colour}] (c) at (-1,-1) {};\n",
    "\\node[vertex,label=right:{Vote}] (v) at (0,-2) {};\n",
    "\\foreach \\source/ \\dest in {r/a, c/j, r/v, c/v,a/v, g/j}\n",
    "        \\path[edge] (\\source) -- (\\dest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Analysis of algorithms\n",
    "\n",
    "### (4 marks) Incremental EM\n",
    "$\\newcommand{\\B}[1]{\\mathbf{#1}}$\n",
    "$\\newcommand{\\Btheta}{\\boldsymbol{\\mathsf{\\theta}}}$\n",
    "$\\newcommand{\\Cond}{\\,|\\,}$\n",
    "\n",
    "Given are $N$ independent data points $\\B{x}_1,\\dots,\\B{x}_N$ and a Gaussian mixture model with corresponding latent variables $\\B{z}_1,\\dots,\\B{z}_N$. As the joint distribution $p(\\B{X}, \\B{Z} \\Cond \\Btheta)$ factorises over the data points, one can develop an incremental form of EM in which only one data point is processed in each E- and M-step if the mixture component is a member of the exponential family. \n",
    "\n",
    "Consider a mixture of Gaussians. The update for the effective number of data points $N_k$ after having seen data point $\\B{x}_m$ can be derived as\n",
    "$$\n",
    "  N_k^\\text{new} = N_k^\\text{old} + \\gamma^\\text{new}(z_{mk}) - \\gamma^\\text{old}(z_{mk})\n",
    "$$\n",
    "\n",
    "1. Define $\\gamma^\\text{new}(z_{mk})$.\n",
    "- Prove the above given result for the update of $N_k$.\n",
    "- Develop the update formulas for the other parameters of the Gaussian mixture model: \n",
    "  - mixture coefficients $\\mathsf{\\pi}_k$, \n",
    "  - means $\\mathsf{\\mu}_k$, and \n",
    "  - covariances $\\mathsf{\\Sigma}_k$.\n",
    "- Define and explain two properties of this incremental EM algorithm which make it preferable method over batch EM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. The mixture defines a complex distribution over $x_n$ in terms of a set of K simpler Gaussian distributions $(\\mathbf{x_n}\\lvert z_{nk} = 1)$. The latent variable $\\mathbf{z_n}$ has a 1-of-K representation where, roughly speaking, $z_k$ equal to 1 if $x_n$ 'was drawn from' the kth Gaussian $(\\mathbf{x_n}\\lvert z_{nk} = 1)$. More precisely, $\\gamma(z_{mk}) = p(z_{mk}= 1 \\lvert \\mathbf{x}_m)$ is the posterior probability that $z_{mk} = 1$ given $x_m$, or the 'responsibility' of the $k^{th}$ component of the mixture for 'explaining' the observation $\\mathbf{x_m}$ (cf. Bishop p432). In incremental EM, the responsibility update for a single point $x_m$ is computed from the current estimates of the parameters $\\mu_k^{old}$ and $\\Sigma^{old}$ and the mixing coefficients $\\pi^{old}$. By Bayes Rule:\n",
    "\\begin{align}\n",
    "\\gamma^{new}(z_{mk}) = \\frac{p(z_{km} = 1)p(\\mathbf{x_m}\\lvert z_{km} = 1)}{p(\\mathbf{x_m})} = \\frac{\\pi_k^{old} \\mathcal{N}(\\mathbf{x_m} \\lvert \\mu^{old}_k,\\Sigma^{old}_k)}{\\sum_{j=1}^k\\pi_j^{old} \\mathcal{N}(\\mathbf{x_m} \\lvert \\mu^{old}_j,\\Sigma^{old}_j)}\n",
    "\\end{align}\n",
    "2. From Bishop 9.18 we have that\n",
    "\\begin{align}\n",
    "N_k^{old} &= \\sum_{n=1}^N \\gamma^{old}(z_{nk})\\\\\n",
    "\\end{align}\n",
    "Updating the responsibilities $\\gamma(z_{mk})$:\n",
    "\\begin{align}\n",
    "N_k^{new} &= \\sum_{n\\neq m}\\big(\\gamma^{old}(z_{nk})\\big) + \\gamma^{new}(z_{mk}) \\\\\n",
    "&= \\sum_{n=1}^N\\big(\\gamma^{old}(z_{nk}) \\big) - \\gamma^{old}(z_{mk}) + \\gamma^{new}(z_{mk})) \\\\\n",
    "&= N_k^{old} + \\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.(i) \n",
    "\\begin{align}\n",
    "\\pi_k^{new} = \\frac{N_k^{new}}{N} &= \\frac{N^{old}_k}{N}+\\frac{\\gamma^{new}(z_{mk})-\\gamma^{old}(z_{mk})}{N}\\\\\n",
    "&= \\pi_{old}+\\frac{\\gamma^{new}(z_{mk})-\\gamma^{old}(z_{mk})}{N}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Again from Bishop 9.18 we have the following two definitions:\n",
    "\\begin{align}\n",
    "\\mu_k^{old} &= \\frac{1}{N_k^{old}}\\sum_{n=1}^{N}\\gamma(z_{nk})x_n\\\\\n",
    "\\end{align}\n",
    "Updating the responsibilities $\\gamma(z_{mk})$:\n",
    "\\begin{align}\n",
    "\\mu^{new}_k &= \\frac{1}{N_k^{new}}\\sum_{n=1}^{N}\\gamma (z_{nk})x_n\\\\\n",
    "&= \\frac{1}{N_k^{new}}\\bigg(\\sum_{n\\neq m}^{N}\\big(\\gamma^{old}(z_{nk})x_n\\big) + \\gamma^{new}(z_{mk})x_m\\bigg)\\\\\n",
    "&= \\frac{1}{N_k^{new}}\\bigg(\\sum_{n=1}^{N}\\big(\\gamma^{old}(z_{nk})x_n\\big) + \\gamma^{new}(z_{mk})x_m - \\gamma^{old}(z_{mk})x_m\\bigg)\\\\\n",
    "&=  \\frac{N_k^{old}}{N_k^{new}}\\mu^{old}_k + \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_k^{new}}x_m\\\\\n",
    "&= \\frac{N_k^{new} - \\gamma_k^{new}(z_{mk}) + \\gamma_k^{old}(z_{mk})}{N_k^{new}}\\mu^{old}_k + \\frac{\\gamma^{new}\n",
    "(z_{mk}) - \\gamma^{old}(z_{mk})}{N_k^{new}}x_m\\\\\n",
    "&= \\mu^{old}_k - \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_k^{new}}\\mu^{old}_k  + \\frac{\\gamma^{new}(z_{mk}) - \\gamma^{old}(z_{mk})}{N_k^{new}}x_m \\\\\n",
    "&= \\mu^{old}_k - \\frac{\\gamma_k^{new}(z_{mk}) - \\gamma_k^{old}(z_{mk})}{N_k^{new}}(x_m - \\mu^{old}_k)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Again, from 9.18 we have that:\n",
    "\\begin{align}\n",
    "\\Sigma_k^{old} &= \\frac{1}{N_k^{old}}\\sum_{n=1}^{N}\\gamma (z_{nk})(x_n-\\mu_k^{old})(x_n-\\mu_k^{old})^T \\\\\n",
    "\\end{align}\n",
    "Updating the responsibilities $\\gamma(z_{mk})$:\n",
    "\\begin{align}\n",
    "\\Sigma_k^{new} &= \\frac{1}{N_k^{new}}\\big(\\sum_{n\\neq m}^{N}\\gamma^{old} (z_{nk})(x_n-\\mu_k^{old})(x_n-\\mu_k^{old})^T + \\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T \\big)\\\\\n",
    "&= \\frac{1}{N_k^{new}}\\big(\\sum_{n\\neq m}^{N}\\gamma^{old} (z_{nk})(x_n-\\mu_k^{new})(x_n-\\mu_k^{new})^T + \\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T \\big)\\\\\n",
    "&= \\frac{1}{N_k^{new}}\\big(\\sum_{n=1}^{N}\\gamma^{old} (z_{nk})(x_n-\\mu_k^{old})(x_n-\\mu_k^{old})^T - \\gamma^{old}(z_{mk})(x_m -\\mu_k^{old})(x_m-\\mu_k^{old})^T + \\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T \\big)\\\\\n",
    "&= \\frac{N_k^{old}}{N_k^{new}}\\Sigma^{old}_k + \\frac{1}{N_k^{new}}\\big(\\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T - \\gamma^{old}(z_{mk})(x_m -\\mu_k^{old})(x_m-\\mu_k^{old})^T\\big)\\\\\n",
    "&= \\Sigma^{old}_k  + \\frac{1}{N_k^{new}}\\big((N_k^{old}-N_k^{new})\\Sigma^{old}_k + \\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T - \\gamma^{old}(z_{mk})(x_m -\\mu_k^{old})(x_m-\\mu_k^{old})^T\\big)\\\\\n",
    "&= \\Sigma^{old}_k  + \\frac{1}{N_k^{new}}\\big(-(\\gamma^{new}-\\gamma^{old})\\Sigma^{old}_k + \\gamma^{new}(z_{mk})(x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T - \\gamma^{old}(z_{mk})(x_m -\\mu_k^{old})(x_m-\\mu_k^{old})^T\\big)\\\\\n",
    "&= \\Sigma^{old}_k  + \\frac{1}{N_k^{new}}\\big(\\gamma^{new}(z_{mk})((x_m -\\mu_k^{new})(x_m-\\mu_k^{new})^T - \\Sigma^{old}_k) - \\gamma^{old}(z_{mk})((x_m -\\mu_k^{old})(x_m-\\mu_k^{old})^T- \\Sigma^{old}_k)\\big)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . *** Properties of Incremental EM ***\n",
    "* If the model (for which we are trying to maximise the likelihood function) is very complex, the E step and/or the M step of the EM algorithm may constitute an intractable computatation, particularly if the number of data points is very large (cf. Bishop p455). This is because the E and M steps of the batch algorithm have time complexity dependent on the number of data points. The incremental algorithm exploits the fact that the joint distribution (over the 'complete' data set) $p(\\B{X}, \\B{Z} \\Cond \\Btheta)$ factorises over the data points, and we can compute 'partial' E and M steps by updating the responsibilities for one data point, and then doing the corresponding M step with this new information. Thus the E and M step have constant time complexity in the number of data points.\n",
    "* The incremental version of EM can often converge faster than the incremental version. This is because we can get good estimates of the parameters of the Gaussians in the mixture, and of the mixing coefficients, after processing a relatively small number of data points. The batch version of the EM algorithm requires the processing of the reponsibilities for all data points before we compute an M step to update the parameters, so it will take much longer to get good estimates of the model paramters if the number of data points is very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3 marks) Sampling from a piecewise exponential distribution\n",
    "\n",
    "In the lecture we mentioned sampling from the *exponential distribution*\n",
    "given by\n",
    "$$\n",
    "  p(y) = \\lambda e^{-\\lambda y}.\n",
    "$$\n",
    "\n",
    "The *piecewise exponential distribution* is defined as\n",
    "$$\n",
    "  p(z) = k_m \\lambda_m e^{-\\lambda_m (z - z_{m})} \n",
    "         \\qquad \\qquad  \n",
    "         \\widehat{z}_{m-1,m} < z \\le \\widehat{z}_{m,m+1}\n",
    "$$\n",
    "where $ \\widehat{z}_{m-1, m} $ is the point of intersection of the tangent \n",
    "lines at $ z_{m-1} $ and $ z_m $,\n",
    "$ \\lambda_m $ is the slope of the tangent at $ z_m $ and \n",
    "$ k_m $ accounts for the corresponding offset.\n",
    "\n",
    "(a) Using the result for sampling from a single distribution,\n",
    "devise an algorithm for sampling from the piecewise \n",
    "distribution.\n",
    "\n",
    "(b) Given a desired distribution $ p(z) $, describe precisely how the \n",
    "piecewise exponential distribution can be used to sample from this\n",
    "distribution.\n",
    "\n",
    "(c) Explain how one can use rejected samples to better \n",
    "approximate the desired distribution $ p(z) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(a) Algorithm for sampling from the piecewise distribtion:***\n",
    "* First, we find the inverse of the cumulative distribution function (CDF) of our piecewise exponential distribution. The inverse of a piecewise function is also defined piecewise, as is the CDF of a piecewise distribution. Hence we need only define the inverse of the CDF piecewise. That means we can state a general form of the CDF for a point lying in the interval z_{i-1,i}, z_{i,i+1}. Our piecewise pdf is defined as follows:\n",
    "\n",
    "$$\n",
    "  p(z) = k_i \\lambda_i e^{-\\lambda_i (z - z_{i})} \n",
    "         \\qquad \\qquad  \n",
    "         z_{i-1,i} < z \\le z_{i,i+1}\n",
    "$$\n",
    "\n",
    "The CDF h(y) is simply the integral:\n",
    "$$\n",
    "h(y) = \\int_{-\\infty}^{y} p(z) dz\n",
    "$$\n",
    "which decomposes piecewise into the definite integral over all intervals less than $z_{i-1,i}, z_{i,i+1}$, and the integral from $z_{i,i+1}$ to y for the interval in which y falls:\n",
    "$$\n",
    "h(y) = \\sum_{j<i} \\int_{z_{j-1,j}}^{z_{j,j+1}} k_j \\lambda_j e^{-\\lambda_j (z - z_{j})}  dz + \\int_{z_{i-1,i}}^{y} k_i \\lambda_i e^{-\\lambda_i (z - z_{i})}  dz \n",
    "         \\qquad \\qquad  \n",
    "         z_{i-1,i} < y \\le z_{i,i+1}\n",
    "$$\n",
    "To simplify notation as we derive the inverse, we denote the sum on the left as $C_j$, which is only dependent on the interval in which $y$ falls and is not explicitly a function of $y$ in the final expression. \n",
    "$$\n",
    "h(y) = C_i + \\int_{z_{i-1,i}}^{y} k_i \\lambda_i e^{-\\lambda_i (z - z_{i})}  dz\n",
    "         \\qquad \\qquad  \n",
    "         z_{i-1,i} < y \\le z_{i,i+1}\n",
    "$$\n",
    "Evaluating the indefinite integral on the right yields:\n",
    "\n",
    "$$\n",
    "h(y) = C_i + D_i - k_i e^{-\\lambda_i (y - z_{i})}\n",
    "         \\qquad \\qquad  \n",
    "         z_{i-1,i} < y \\le z_{i,i+1}\n",
    "$$\n",
    "\n",
    "where $ D_i = k_i e^{-\\lambda_i (z_{i-1,i} - z_{i})} $ which does not depend explicitly on y. Again, for simplicity, we absorb it into $C_i$, which yields:\n",
    "\n",
    "$$\n",
    "h(y) = C_i - k_i e^{-\\lambda_i (y - z_{i})}\n",
    "         \\qquad \\qquad  \n",
    "         z_{i-1,i} < y \\le z_{i,i+1}\n",
    "$$\n",
    "\n",
    "We now take the inverse of the CDF h(y), to get our random variable y in terms of the value of the CDF z = h(y). The inverse has a simple form similar to that of the inverse of the normal exponential distribution:\n",
    "\n",
    "$$\n",
    "y  = z_i - \\frac{1}{\\lambda_i}\\ln{\\frac{C_i - z}{k_i}}\n",
    "         \\qquad \\qquad  \n",
    "         h(z_{i-1,i}) < z \\le h(z_{i,i+1})\n",
    "$$\n",
    "\n",
    "* If $z$ is sampled from the uniform distribution $\\mathcal{U}(0,1)$, then the random variable $y$ will be distributed according to the piecewise exponential distribution. Hence we simply use a random number generator to sample a value of $z$ and compute the value of $y$ using the above formula for the inverse of the CDF of the piecewise exponential. $y$ will be a sample from the piecewise exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (b) Using the piecewise exponential distribution to sample from a desired distribution p(z) (cf Bishop p529) **\n",
    "\n",
    "The piecewise exponential can be used as an envelope distribution in rejection sampling. The typical situation in which we would use rejection sampling is when it is difficult to sample directly from the desired distribution p(z), but it is easy to evaluate p(z) to within some constant multiplicative factor (a normalizing constant $Z_p$). More precisely, when it is easy to evaluate $\\bar{p}(z) = Z_p p(z)$. Rejection sampling entails sampling from a simpler envelope (or proposal) distribution $q(z)$ that multiplicatively dominates the (unnormalised) desired distribution $\\bar{p}(z)$.\n",
    "* **Rejection Sampling in general **\n",
    "    * First we sample a random number z_0 uniformly from [0,1] using a random number generator, and use the inverse of the CDF of q(z) in the normal way to sample from the distribution q(z).\n",
    "    * Next, we sample a number u_0 uniformly from $[0,kq(z)]$, where k is some constant such that $kq(z) > p(z) \\forall z$.\n",
    "    * Now, if u_0 >  $\\bar{p}(z)$ we reject the sample (hence the name).\n",
    "    * The unrejected samples are uniformly distributed under the curve $\\bar{p}(z)$, and so will be distributed according to the desired distribution p(z).\n",
    "* ** Using the piecewise exponential as the envelope distribution **\n",
    "    * The piecewise exponential is used in cases where it is difficult to find a suitable analytic form for the envelope distribution $q(z)$. In particular, if the desired distribution $p(z)$ is log concave, we can construct the envelope function piecewise as follows (cf. Bishop p 530):\n",
    "    * We take a grid of points $z_i$ and evaluate $ln(\\bar{p}(z_i))$, and its gradient at each gridpoint.\n",
    "    * Each pair (function value and gradient value) specify the tangent to $ln(\\bar{p}(z_i))$ at that point $z_i$. We compute the points where the tangents intersect, labelled  z_{i-1}, z_{i}. These define the bounds of the intervals over which our piecewise envelope distribution is defined. The tangent segments are the log of our piecewise envelope distribution.\n",
    "    * Since each of these tangent segments is a straight line in log space, the envelope distribution so constructed is the piecewise exponential. \n",
    "    * We use it to perform rejection sampling as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** (c) Adaptive Rejection Sampling with the Piecewise exponential (cf Bishop p529) **\n",
    "*  Since our envelope function is constructed piecewise, we are able to continue to refine it by defining tighter and tighter intervals, such that it hugs the desired distribution more and more as we sample. We do this in a way that is nicely integrated with the rejection sampling algorithm\n",
    "* In order to reject a sample at point z_j, we must have evaluated $\\bar{p}(z_j)$. We therefore need only compute the log and the log gradient at this point and we have defined a new tangent: the tangent to $\\bar{p}(z_j)$ at this new gridpoint $z_j$. We then compute the intersections of this tangent with those of the adjacent gridpoints, and we have refined one of our intervals into two. The resultant function segments more closely hug $\\bar{p}(z_j)$ (since in the limit, taking tangents at finer and finer gridscales will perfectly approximate the function). Thus the probability that future samples will be rejected is reduced by the adaptation of the piecewise function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
